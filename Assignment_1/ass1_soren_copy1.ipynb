{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import os\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn import linear_model \n",
    "import warnings\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "# seaborn can be used to \"prettify\" default matplotlib plots by importing and setting as default\n",
    "import seaborn as sns\n",
    "sns.set() # Set searborn as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from txt file, headers have spaces in them..\n",
    "data = pd.read_csv('case1Data.txt', sep=\",\")\n",
    "x_new = pd.read_csv('case1Data_Xnew.txt', sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorative data analysis\n",
    "\n",
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (100, 101)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x_ 1</th>\n",
       "      <th>x_ 2</th>\n",
       "      <th>x_ 3</th>\n",
       "      <th>x_ 4</th>\n",
       "      <th>x_ 5</th>\n",
       "      <th>x_ 6</th>\n",
       "      <th>x_ 7</th>\n",
       "      <th>x_ 8</th>\n",
       "      <th>x_ 9</th>\n",
       "      <th>...</th>\n",
       "      <th>x_91</th>\n",
       "      <th>x_92</th>\n",
       "      <th>x_93</th>\n",
       "      <th>x_94</th>\n",
       "      <th>x_95</th>\n",
       "      <th>C_ 1</th>\n",
       "      <th>C_ 2</th>\n",
       "      <th>C_ 3</th>\n",
       "      <th>C_ 4</th>\n",
       "      <th>C_ 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.685036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.978178</td>\n",
       "      <td>-1.504027</td>\n",
       "      <td>-14.986790</td>\n",
       "      <td>0.651581</td>\n",
       "      <td>-37.878245</td>\n",
       "      <td>16.213807</td>\n",
       "      <td>-3.965437</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.032047</td>\n",
       "      <td>6.319109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.197445</td>\n",
       "      <td>-9.913837</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>I</td>\n",
       "      <td>K</td>\n",
       "      <td>J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60.844445</td>\n",
       "      <td>26.204958</td>\n",
       "      <td>11.185962</td>\n",
       "      <td>-8.247602</td>\n",
       "      <td>-3.875866</td>\n",
       "      <td>-11.329702</td>\n",
       "      <td>2.637900</td>\n",
       "      <td>-39.617497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.442164</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.437474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-12.880342</td>\n",
       "      <td>25.924603</td>\n",
       "      <td>-7.015803</td>\n",
       "      <td>K</td>\n",
       "      <td>H</td>\n",
       "      <td>J</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.062345</td>\n",
       "      <td>18.560873</td>\n",
       "      <td>11.355342</td>\n",
       "      <td>-9.370161</td>\n",
       "      <td>-3.745315</td>\n",
       "      <td>-16.292421</td>\n",
       "      <td>-1.831774</td>\n",
       "      <td>-37.584605</td>\n",
       "      <td>14.411348</td>\n",
       "      <td>-2.376175</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.291624</td>\n",
       "      <td>2.664209</td>\n",
       "      <td>-15.803518</td>\n",
       "      <td>24.293073</td>\n",
       "      <td>-14.795709</td>\n",
       "      <td>I</td>\n",
       "      <td>H</td>\n",
       "      <td>G</td>\n",
       "      <td>H</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-8.139839</td>\n",
       "      <td>31.896496</td>\n",
       "      <td>10.069040</td>\n",
       "      <td>-9.484426</td>\n",
       "      <td>-1.512786</td>\n",
       "      <td>-11.421909</td>\n",
       "      <td>-5.649180</td>\n",
       "      <td>-42.830037</td>\n",
       "      <td>18.403197</td>\n",
       "      <td>-7.791762</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.253436</td>\n",
       "      <td>5.004891</td>\n",
       "      <td>-13.879328</td>\n",
       "      <td>25.954304</td>\n",
       "      <td>-9.365574</td>\n",
       "      <td>NaN</td>\n",
       "      <td>H</td>\n",
       "      <td>K</td>\n",
       "      <td>K</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.994792</td>\n",
       "      <td>25.400286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.600959</td>\n",
       "      <td>-2.615988</td>\n",
       "      <td>-15.455676</td>\n",
       "      <td>-4.950816</td>\n",
       "      <td>-39.932456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.341165</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.101262</td>\n",
       "      <td>-16.938532</td>\n",
       "      <td>22.611184</td>\n",
       "      <td>-17.240003</td>\n",
       "      <td>G</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>K</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           y       x_ 1       x_ 2      x_ 3      x_ 4       x_ 5      x_ 6  \\\n",
       "0  90.685036        NaN        NaN -3.978178 -1.504027 -14.986790  0.651581   \n",
       "1  60.844445  26.204958  11.185962 -8.247602 -3.875866 -11.329702  2.637900   \n",
       "2  18.062345  18.560873  11.355342 -9.370161 -3.745315 -16.292421 -1.831774   \n",
       "3  -8.139839  31.896496  10.069040 -9.484426 -1.512786 -11.421909 -5.649180   \n",
       "4  -4.994792  25.400286        NaN -3.600959 -2.615988 -15.455676 -4.950816   \n",
       "\n",
       "        x_ 7       x_ 8      x_ 9  ...       x_91      x_92       x_93  \\\n",
       "0 -37.878245  16.213807 -3.965437  ...  -7.032047  6.319109        NaN   \n",
       "1 -39.617497        NaN -2.442164  ... -13.437474       NaN -12.880342   \n",
       "2 -37.584605  14.411348 -2.376175  ... -12.291624  2.664209 -15.803518   \n",
       "3 -42.830037  18.403197 -7.791762  ... -13.253436  5.004891 -13.879328   \n",
       "4 -39.932456        NaN -7.341165  ...        NaN  5.101262 -16.938532   \n",
       "\n",
       "        x_94       x_95   C_ 1   C_ 2   C_ 3   C_ 4   C_ 5  \n",
       "0  26.197445  -9.913837      H      H      I      K      J  \n",
       "1  25.924603  -7.015803      K      H      J      G      G  \n",
       "2  24.293073 -14.795709      I      H      G      H      G  \n",
       "3  25.954304  -9.365574    NaN      H      K      K      G  \n",
       "4  22.611184 -17.240003      G      H      H      K      H  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape\", data.shape)\n",
    "# convert datatypes\n",
    "data = data.apply(lambda x: x.astype('float64') if 'x' in x.name else x)\n",
    "data = data.apply(lambda x: x.astype('category') if 'C' in x.name else x)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x_ 1</th>\n",
       "      <th>x_ 2</th>\n",
       "      <th>x_ 3</th>\n",
       "      <th>x_ 4</th>\n",
       "      <th>x_ 5</th>\n",
       "      <th>x_ 6</th>\n",
       "      <th>x_ 7</th>\n",
       "      <th>x_ 8</th>\n",
       "      <th>x_ 9</th>\n",
       "      <th>...</th>\n",
       "      <th>x_86</th>\n",
       "      <th>x_87</th>\n",
       "      <th>x_88</th>\n",
       "      <th>x_89</th>\n",
       "      <th>x_90</th>\n",
       "      <th>x_91</th>\n",
       "      <th>x_92</th>\n",
       "      <th>x_93</th>\n",
       "      <th>x_94</th>\n",
       "      <th>x_95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-9.449225</td>\n",
       "      <td>24.152278</td>\n",
       "      <td>11.535373</td>\n",
       "      <td>-9.744737</td>\n",
       "      <td>-4.060555</td>\n",
       "      <td>-16.392856</td>\n",
       "      <td>-2.810344</td>\n",
       "      <td>-38.684160</td>\n",
       "      <td>15.323868</td>\n",
       "      <td>-4.836150</td>\n",
       "      <td>...</td>\n",
       "      <td>17.637259</td>\n",
       "      <td>0.652821</td>\n",
       "      <td>12.151878</td>\n",
       "      <td>-7.467135</td>\n",
       "      <td>-8.040902</td>\n",
       "      <td>-14.477416</td>\n",
       "      <td>4.552843</td>\n",
       "      <td>-16.015159</td>\n",
       "      <td>23.371891</td>\n",
       "      <td>-13.343786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>54.637475</td>\n",
       "      <td>3.226483</td>\n",
       "      <td>2.616437</td>\n",
       "      <td>3.201830</td>\n",
       "      <td>2.605455</td>\n",
       "      <td>2.820285</td>\n",
       "      <td>2.946260</td>\n",
       "      <td>2.235738</td>\n",
       "      <td>3.036549</td>\n",
       "      <td>2.825773</td>\n",
       "      <td>...</td>\n",
       "      <td>2.747872</td>\n",
       "      <td>3.001337</td>\n",
       "      <td>3.026242</td>\n",
       "      <td>2.808379</td>\n",
       "      <td>2.672240</td>\n",
       "      <td>3.087054</td>\n",
       "      <td>2.670058</td>\n",
       "      <td>2.759980</td>\n",
       "      <td>2.781380</td>\n",
       "      <td>3.125821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-178.046246</td>\n",
       "      <td>16.031508</td>\n",
       "      <td>4.278967</td>\n",
       "      <td>-16.157991</td>\n",
       "      <td>-10.456794</td>\n",
       "      <td>-24.271015</td>\n",
       "      <td>-10.201020</td>\n",
       "      <td>-43.827753</td>\n",
       "      <td>8.435083</td>\n",
       "      <td>-11.078308</td>\n",
       "      <td>...</td>\n",
       "      <td>11.211716</td>\n",
       "      <td>-6.232716</td>\n",
       "      <td>4.481362</td>\n",
       "      <td>-13.284533</td>\n",
       "      <td>-16.702016</td>\n",
       "      <td>-21.322369</td>\n",
       "      <td>-1.395252</td>\n",
       "      <td>-21.761067</td>\n",
       "      <td>16.877332</td>\n",
       "      <td>-19.410096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-44.109740</td>\n",
       "      <td>21.718763</td>\n",
       "      <td>9.735326</td>\n",
       "      <td>-11.824124</td>\n",
       "      <td>-5.868863</td>\n",
       "      <td>-17.906581</td>\n",
       "      <td>-4.655860</td>\n",
       "      <td>-40.320246</td>\n",
       "      <td>12.998737</td>\n",
       "      <td>-6.770507</td>\n",
       "      <td>...</td>\n",
       "      <td>15.798499</td>\n",
       "      <td>-1.607624</td>\n",
       "      <td>10.239859</td>\n",
       "      <td>-9.517063</td>\n",
       "      <td>-9.918741</td>\n",
       "      <td>-16.347168</td>\n",
       "      <td>2.560855</td>\n",
       "      <td>-17.887639</td>\n",
       "      <td>21.474290</td>\n",
       "      <td>-15.972494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-5.108532</td>\n",
       "      <td>24.039863</td>\n",
       "      <td>11.204954</td>\n",
       "      <td>-9.711051</td>\n",
       "      <td>-3.823393</td>\n",
       "      <td>-16.290923</td>\n",
       "      <td>-2.431349</td>\n",
       "      <td>-38.787664</td>\n",
       "      <td>15.739363</td>\n",
       "      <td>-4.969147</td>\n",
       "      <td>...</td>\n",
       "      <td>17.573367</td>\n",
       "      <td>0.636956</td>\n",
       "      <td>12.737008</td>\n",
       "      <td>-6.902784</td>\n",
       "      <td>-8.000845</td>\n",
       "      <td>-14.268092</td>\n",
       "      <td>4.749562</td>\n",
       "      <td>-16.170375</td>\n",
       "      <td>23.697288</td>\n",
       "      <td>-12.975518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>24.499702</td>\n",
       "      <td>26.422000</td>\n",
       "      <td>13.360494</td>\n",
       "      <td>-7.684899</td>\n",
       "      <td>-2.458013</td>\n",
       "      <td>-14.732024</td>\n",
       "      <td>-0.953774</td>\n",
       "      <td>-37.246405</td>\n",
       "      <td>17.485969</td>\n",
       "      <td>-2.585693</td>\n",
       "      <td>...</td>\n",
       "      <td>19.409453</td>\n",
       "      <td>2.920330</td>\n",
       "      <td>13.810049</td>\n",
       "      <td>-5.718319</td>\n",
       "      <td>-5.983640</td>\n",
       "      <td>-13.100169</td>\n",
       "      <td>6.476377</td>\n",
       "      <td>-14.151908</td>\n",
       "      <td>25.497565</td>\n",
       "      <td>-11.206957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>113.446867</td>\n",
       "      <td>31.991717</td>\n",
       "      <td>18.573992</td>\n",
       "      <td>-1.861803</td>\n",
       "      <td>2.531620</td>\n",
       "      <td>-9.921926</td>\n",
       "      <td>4.792236</td>\n",
       "      <td>-31.823017</td>\n",
       "      <td>24.255647</td>\n",
       "      <td>1.294652</td>\n",
       "      <td>...</td>\n",
       "      <td>24.234257</td>\n",
       "      <td>6.492363</td>\n",
       "      <td>19.442490</td>\n",
       "      <td>-0.294956</td>\n",
       "      <td>-1.973244</td>\n",
       "      <td>-6.163989</td>\n",
       "      <td>10.183673</td>\n",
       "      <td>-8.911256</td>\n",
       "      <td>29.450024</td>\n",
       "      <td>-6.630455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                y       x_ 1       x_ 2       x_ 3       x_ 4       x_ 5  \\\n",
       "count  100.000000  84.000000  87.000000  81.000000  82.000000  85.000000   \n",
       "mean    -9.449225  24.152278  11.535373  -9.744737  -4.060555 -16.392856   \n",
       "std     54.637475   3.226483   2.616437   3.201830   2.605455   2.820285   \n",
       "min   -178.046246  16.031508   4.278967 -16.157991 -10.456794 -24.271015   \n",
       "25%    -44.109740  21.718763   9.735326 -11.824124  -5.868863 -17.906581   \n",
       "50%     -5.108532  24.039863  11.204954  -9.711051  -3.823393 -16.290923   \n",
       "75%     24.499702  26.422000  13.360494  -7.684899  -2.458013 -14.732024   \n",
       "max    113.446867  31.991717  18.573992  -1.861803   2.531620  -9.921926   \n",
       "\n",
       "            x_ 6       x_ 7       x_ 8       x_ 9  ...       x_86       x_87  \\\n",
       "count  91.000000  89.000000  85.000000  87.000000  ...  84.000000  92.000000   \n",
       "mean   -2.810344 -38.684160  15.323868  -4.836150  ...  17.637259   0.652821   \n",
       "std     2.946260   2.235738   3.036549   2.825773  ...   2.747872   3.001337   \n",
       "min   -10.201020 -43.827753   8.435083 -11.078308  ...  11.211716  -6.232716   \n",
       "25%    -4.655860 -40.320246  12.998737  -6.770507  ...  15.798499  -1.607624   \n",
       "50%    -2.431349 -38.787664  15.739363  -4.969147  ...  17.573367   0.636956   \n",
       "75%    -0.953774 -37.246405  17.485969  -2.585693  ...  19.409453   2.920330   \n",
       "max     4.792236 -31.823017  24.255647   1.294652  ...  24.234257   6.492363   \n",
       "\n",
       "            x_88       x_89       x_90       x_91       x_92       x_93  \\\n",
       "count  86.000000  88.000000  83.000000  79.000000  87.000000  87.000000   \n",
       "mean   12.151878  -7.467135  -8.040902 -14.477416   4.552843 -16.015159   \n",
       "std     3.026242   2.808379   2.672240   3.087054   2.670058   2.759980   \n",
       "min     4.481362 -13.284533 -16.702016 -21.322369  -1.395252 -21.761067   \n",
       "25%    10.239859  -9.517063  -9.918741 -16.347168   2.560855 -17.887639   \n",
       "50%    12.737008  -6.902784  -8.000845 -14.268092   4.749562 -16.170375   \n",
       "75%    13.810049  -5.718319  -5.983640 -13.100169   6.476377 -14.151908   \n",
       "max    19.442490  -0.294956  -1.973244  -6.163989  10.183673  -8.911256   \n",
       "\n",
       "            x_94       x_95  \n",
       "count  85.000000  90.000000  \n",
       "mean   23.371891 -13.343786  \n",
       "std     2.781380   3.125821  \n",
       "min    16.877332 -19.410096  \n",
       "25%    21.474290 -15.972494  \n",
       "50%    23.697288 -12.975518  \n",
       "75%    25.497565 -11.206957  \n",
       "max    29.450024  -6.630455  \n",
       "\n",
       "[8 rows x 96 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in data.columns if x != 'y']   \n",
    "numerical_features = data[features].select_dtypes(include=['float64']).columns\n",
    "categorical_features = data[features].select_dtypes(include=['category']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values\n",
    "* Lots of them, how to handle?\n",
    "* Assume they are missing at random\n",
    "* Deletion, imputation or model based?\n",
    "* Deletion: Will lose too much information\n",
    "* Imputation: Perhaps, but chose carefully. Look at histogram of each feature for continous. For categorical, consider just making NaN into a category in itself, or remove?\n",
    "* SciKit Learns IterativeImputation seem to be working very well and catches trends in data while keeping the same overall summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plot of variables with missing values\n",
    "missing = data.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "print(\"Number of features with missing values =\", missing.shape[0])\n",
    "print(\"Total number of missing values =\", missing.sum())\n",
    "missing.sort_values(inplace=True)\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.title(\"Number of missing values in each feature\")\n",
    "plt.xlabel(\"Feature name\")\n",
    "plt.ylabel(\"Number of missing values\")\n",
    "sns.barplot(x=missing.index, y=missing.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig('Plots/train_num_missing_values.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of each feature with missing values\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, feature in enumerate(features):\n",
    "    plt.subplot(10, 10, i+1)\n",
    "    plt.title(feature)\n",
    "    sns.histplot(data[feature], kde=True)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Plots/train_histograms.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize development of each numerical feature over time in one plot\n",
    "plt.figure(figsize=(20, 40))\n",
    "for i, feature in enumerate(features):\n",
    "    plt.subplot(20, 5, i+1)\n",
    "    plt.title(feature)\n",
    "    plt.plot(data[feature])\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Plots/train_development_missing_values.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values for numerical features with interpolation\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "data_imputed = data.copy()\n",
    "data_imputed[numerical_features] = imputer.fit_transform(data[numerical_features])\n",
    "\n",
    "# plot the data after imputation\n",
    "plt.figure(figsize=(20, 40))\n",
    "for i, feature in enumerate(features):\n",
    "    plt.subplot(20, 5, i+1)\n",
    "    plt.title(feature)\n",
    "    plt.plot(data_imputed[feature])\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Plots/train_development_iterative_imputation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a lot of missing values. Let's try to remove rows that have more than 15 missing values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Development of y over time\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.title(\"Development of y\")\n",
    "plt.xlabel(\"Observation\")\n",
    "plt.ylabel(\"Value of y\")\n",
    "sns.lineplot(x=data.index, y=data.y)\n",
    "# min and max\n",
    "plt.text(data.y.idxmax(), data.y.max(), f'Max: {round(data.y.max(),2)}', c = 'r')\n",
    "plt.text(data.y.idxmin(), data.y.min(), f'Min: {round(data.y.min(),2)}', c = 'r')\n",
    "# mean and median\n",
    "plt.axhline(data.y.mean(), color='r', linestyle='--', label='Mean')\n",
    "plt.text(-5, data.y.mean()-14, f'Mean: {round(data.y.mean(),2)}', c = 'r')\n",
    "plt.axhline(data.y.median(), color='g', linestyle='--', label='Median')\n",
    "plt.text(-5, data.y.median()+4, f'Median: {round(data.y.median(),2)}', c = 'g')\n",
    "plt.legend()\n",
    "plt.savefig('Plots/train_y_development.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection\n",
    "* How to handle numerical vs categorical features? 1-hot encode categorical features perhaps?\n",
    "* Which techniques to use?\n",
    "* Regularization, Ridge Regression, Shrink parameters. How to chose optimal Lambda value? \n",
    "* Ragularization, Lasso Regression, Remove parameters. How to chose optimal Lambda value? \n",
    "* **Elastic net, mix of the 2 above.** How to chose optimal lambda and alpha values?\n",
    "* Forward vs Backward Selection\n",
    "* Clustering of features\n",
    "* PCA\n",
    "* Feature assessment, Bonferroni Correction, FDR, Benjamini Hochbergs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation analysis\n",
    "data_num = data.select_dtypes(include=['float64'])\n",
    "corr = data_num.corr()\n",
    "# sort corr by y\n",
    "corr = corr.sort_values(by='y', ascending=False)\n",
    "# barplot of correlation with y\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.title(\"Correlation with y\")\n",
    "plt.xlabel(\"Feature name\")\n",
    "plt.ylabel(\"Correlation with y\")\n",
    "sns.barplot(x=corr.index, y=corr.y, hue=corr.y, dodge=False)\n",
    "plt.legend(loc='upper right', title='Correlation with y')\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig('Plots/train_correlation_y.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter top 20 correlated features with y\n",
    "top20_corr = corr.y[1:21]\n",
    "# plot the correlation matrix between these features\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.title(\"Correlation matrix of top 20 features correlated with y\")\n",
    "sns.heatmap(data_num[top20_corr.index].corr(), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.savefig('Plots/train_correlation_matrix_top20.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of y with top 20 correlated features in 5x4 subplot\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.suptitle(\"Scatter plot of y with top 20 correlated features\")\n",
    "for i, col in enumerate(top20_corr.index):\n",
    "    plt.subplot(5, 4, i+1)\n",
    "    sns.scatterplot(x=data[col], y=data.y)\n",
    "    plt.title(f\"y vs {col}\")\n",
    "    # fit line\n",
    "    sns.regplot(x=data[col], y=data.y, scatter=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Plots/train_scatter_with_y_top20.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cat = data.select_dtypes(include=['category'])\n",
    "# subplot for each categorical variable, countplot of y\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 8))\n",
    "fig.suptitle(\"Countplot of y for each categorical variable\")\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < data_cat.shape[1]:\n",
    "        sns.countplot(x=data_cat.iloc[:, i], ax=ax)\n",
    "        ax.set_title(data_cat.columns[i])\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.set_ylabel(\"Y count\")\n",
    "    else:\n",
    "        fig.delaxes(ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "\n",
    "* Binary / onehot encoding of categorical features\n",
    "* Train and validation on imputed data\n",
    "* Cross Validation\n",
    "* Standardize in each cv fold\n",
    "* Train Elastic Net model\n",
    "* RMSE, Accuracy, R^2\n",
    "* Global Interpretability. Beta values with t-statistic for linear regression. DecisionTree Sklearn FeatureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First version\n",
    "* Iterative Imputer for missing values\n",
    "* Binary Encoder for categorical features\n",
    "* All features\n",
    "* Elastic Net\n",
    "* Results:\n",
    "* MSE: 243.33564098079373\n",
    "* R2: 0.914663679874118\n",
    "* MAE: 12.753609579215079\n",
    "* RMSE: 15.599219242667042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sorenbendtsen/opt/anaconda3/envs/model_based/lib/python3.11/site-packages/sklearn/impute/_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "data_imputed = data.copy()\n",
    "data_imputed[numerical_features] = imputer.fit_transform(data[numerical_features])\n",
    "\n",
    "# Binary encoding of categorical features\n",
    "encoder = ce.BinaryEncoder(cols=categorical_features)\n",
    "data_encoded = encoder.fit_transform(data_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "X = data_encoded.drop(columns='y')\n",
    "y = data_encoded.y\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Standardize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 108)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of y: -178.046246 96.821336\n"
     ]
    }
   ],
   "source": [
    "# print range of X_train_scaled\n",
    "print(\"Range of y:\", y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection, alpha and L1 ratio for ElasticNet\n",
    "alpha = np.linspace(0, 4, num=20)\n",
    "l1_ratio = np.linspace(0, 1, num=20)\n",
    "\n",
    "with warnings.catch_warnings(): # done to disable all the convergence warnings from elastic net\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    # plot MSE for different alpha and l1_ratio\n",
    "    mse = np.zeros((len(alpha), len(l1_ratio)))\n",
    "    for i, a in enumerate(alpha):\n",
    "        for j, l in enumerate(l1_ratio):\n",
    "            model = ElasticNet(alpha=a, l1_ratio=l)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_val_scaled)\n",
    "            mse[i, j] = mean_squared_error(y_val, y_pred)\n",
    "\n",
    "# print best alpha and l1_ratio\n",
    "min_mse = np.min(mse)\n",
    "min_mse_idx = np.where(mse == min_mse)\n",
    "best_alpha = alpha[min_mse_idx[0][0]]\n",
    "best_l1_ratio = l1_ratio[min_mse_idx[1][0]]\n",
    "print(f\"Best alpha = {best_alpha}\")\n",
    "print(f\"Best l1_ratio (Lasso) = {best_l1_ratio}\")\n",
    "print(f\"Minimum MSE = {min_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet = ElasticNet(alpha=best_alpha, l1_ratio=best_l1_ratio)\n",
    "enet.fit(X_train_scaled, y_train)\n",
    "y_pred = enet.predict(X_val_scaled)\n",
    "print(\"MSE:\", mean_squared_error(y_val, y_pred))\n",
    "print(\"R2:\", r2_score(y_val, y_pred))\n",
    "print(\"MAE:\", mean_absolute_error(y_val, y_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_val, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "coef = pd.DataFrame(enet.coef_, index=X.columns, columns=['Coefficient'])\n",
    "# sort by absolute value\n",
    "coef['abs'] = coef.Coefficient.abs()\n",
    "coef = coef.sort_values(by='abs', ascending=False).drop(columns='abs')\n",
    "# plot the coefficients\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.title(\"ElasticNet coefficients - Feature Selection\")\n",
    "plt.xlabel(\"Feature name\")\n",
    "plt.ylabel(\"Coefficient\")\n",
    "sns.barplot(x=coef.index, y=coef.Coefficient)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation, y_pred vs y_test\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"y_pred vs y_val\")\n",
    "plt.xlabel(\"Observation\")\n",
    "plt.ylabel(\"Value of y\")\n",
    "# dot\n",
    "plt.scatter(y_val.index, y_val, label='y_val')\n",
    "plt.scatter(y_val.index, y_pred, label='y_pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2\n",
    "* Add Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "mse = np.zeros(5)\n",
    "r2 = np.zeros(5)\n",
    "mae = np.zeros(5)\n",
    "rmse = np.zeros(5)\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    enet = ElasticNet(alpha=best_alpha, l1_ratio=best_l1_ratio)\n",
    "    enet.fit(X_train_scaled, y_train)\n",
    "    y_pred = enet.predict(X_val_scaled)\n",
    "    mse[i] = mean_squared_error(y_val, y_pred)\n",
    "    r2[i] = r2_score(y_val, y_pred)\n",
    "    mae[i] = mean_absolute_error(y_val, y_pred)\n",
    "    rmse[i] = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    i += 1\n",
    "print(\"MSE:\", mse.mean())\n",
    "print(\"R2:\", r2.mean())\n",
    "print(\"MAE:\", mae.mean())\n",
    "print(\"RMSE:\", rmse.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 3\n",
    "* Lasso regression\n",
    "* Results: Same as Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso regression, chose lambda\n",
    "alpha_lasso = np.linspace(0.1, 1, num=20)\n",
    "mse = np.zeros(len(alpha_lasso))\n",
    "for i, l in enumerate(alpha_lasso):\n",
    "    model = linear_model.Lasso(alpha=l)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_val_scaled)\n",
    "    mse[i] = mean_squared_error(y_val, y_pred)\n",
    "\n",
    "# print best alpha\n",
    "min_mse = np.min(mse)\n",
    "min_mse_idx = np.where(mse == min_mse)\n",
    "best_alpha = alpha_lasso[min_mse_idx[0][0]]\n",
    "print(f\"Best alpha = {best_alpha}\")\n",
    "print(f\"Minimum MSE = {min_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without cross validation\n",
    "lasso = linear_model.Lasso(alpha=best_alpha)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "y_pred = lasso.predict(X_val_scaled)\n",
    "print(\"MSE:\", mean_squared_error(y_val, y_pred))\n",
    "print(\"R2:\", r2_score(y_val, y_pred))\n",
    "print(\"MAE:\", mean_absolute_error(y_val, y_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_val, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "mse = np.zeros(5)\n",
    "r2 = np.zeros(5)\n",
    "mae = np.zeros(5)\n",
    "rmse = np.zeros(5)\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    lasso = linear_model.Lasso(alpha=best_alpha)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    y_pred = lasso.predict(X_val_scaled)\n",
    "    mse[i] = mean_squared_error(y_val, y_pred)\n",
    "    r2[i] = r2_score(y_val, y_pred)\n",
    "    mae[i] = mean_absolute_error(y_val, y_pred)\n",
    "    rmse[i] = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    i += 1\n",
    "print(\"MSE:\", mse.mean())\n",
    "print(\"R2:\", r2.mean())\n",
    "print(\"MAE:\", mae.mean())\n",
    "print(\"RMSE:\", rmse.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 4\n",
    "* One hot encoding\n",
    "* Sligtly worse than binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "encoder = ce.OneHotEncoder(cols=categorical_features)\n",
    "data_encoded = encoder.fit_transform(data_imputed)\n",
    "\n",
    "X = data_encoded.drop(columns='y')\n",
    "y = data_encoded.y\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "mse = np.zeros(5)\n",
    "r2 = np.zeros(5)\n",
    "mae = np.zeros(5)\n",
    "rmse = np.zeros(5)\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    lasso = linear_model.Lasso(alpha=best_alpha)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    y_pred = lasso.predict(X_val_scaled)\n",
    "    mse[i] = mean_squared_error(y_val, y_pred)\n",
    "    r2[i] = r2_score(y_val, y_pred)\n",
    "    mae[i] = mean_absolute_error(y_val, y_pred)\n",
    "    rmse[i] = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    i += 1\n",
    "print(\"MSE:\", mse.mean())\n",
    "print(\"R2:\", r2.mean())\n",
    "print(\"MAE:\", mae.mean())\n",
    "print(\"RMSE:\", rmse.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 5\n",
    "* Lasso feature selection\n",
    "* Linear model\n",
    "* Binary encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 345.3027000432613\n",
      "R2: 0.8789044562791812\n",
      "MAE: 15.587951679690496\n",
      "RMSE: 18.582322245706035\n"
     ]
    }
   ],
   "source": [
    "# Lasso regression, non linear polynomial model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# lasso regression for feature selection\n",
    "lasso = linear_model.Lasso(alpha=best_alpha)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "y_pred = lasso.predict(X_val_scaled)\n",
    "\n",
    "# drop features with zero coefficient\n",
    "X_train_lasso = X_train_scaled[:, lasso.coef_ != 0]\n",
    "X_val_lasso = X_val_scaled[:, lasso.coef_ != 0]\n",
    "\n",
    "# linear model\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_train_lasso, y_train)\n",
    "y_pred = model.predict(X_val_lasso)\n",
    "print(\"MSE:\", mean_squared_error(y_val, y_pred))\n",
    "print(\"R2:\", r2_score(y_val, y_pred))\n",
    "print(\"MAE:\", mean_absolute_error(y_val, y_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_val, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data, X New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape\", x_new.shape)\n",
    "# convert datatypes\n",
    "x_new = x_new.apply(lambda x: x.astype('float64') if 'x' in x.name else x)\n",
    "x_new = x_new.apply(lambda x: x.astype('category') if 'C' in x.name else x)\n",
    "x_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plot of variables with missing values\n",
    "missing = x_new.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "print(\"Number of features with missing values\", missing.shape[0])\n",
    "missing.sort_values(inplace=True)\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.title(\"Number of missing values in each feature\")\n",
    "plt.xlabel(\"Feature name\")\n",
    "plt.ylabel(\"Number of missing values\")\n",
    "sns.barplot(x=missing.index, y=missing.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cat_new = x_new.select_dtypes(include=['category'])\n",
    "# subplot for each categorical variable, countplot of y\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 8))\n",
    "fig.suptitle(\"Countplot of y for each categorical variable\")\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < data_cat_new.shape[1]:\n",
    "        sns.countplot(x=data_cat_new.iloc[:, i], ax=ax)\n",
    "        ax.set_title(data_cat_new.columns[i])\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.set_ylabel(\"Y count\")\n",
    "    else:\n",
    "        fig.delaxes(ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "* Cross Validation, how to chose optimal K?\n",
    "* Center and standardize within each fold\n",
    "* Regression with Elastic Net\n",
    "* Estimate prediction error RMSE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('DTU')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "17622c309c226b43e74d64cfd161f2020bc7390f6a3b894c2ae3b9ab7b589c99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
